{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532bedf9-a03f-4767-9193-6b84aed3ae2c",
   "metadata": {},
   "source": [
    "## Analiza danych tekstowych - text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed89bf-2360-48f7-a8f7-d70dffb44a68",
   "metadata": {},
   "source": [
    "Pobierz z wikipedii treść 11 artykułów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0e7fa-6116-4d71-a3e1-ffe2117b9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc73cf-2ed1-4b71-8967-2b3d3467c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "\n",
    "wiki_url = \"http://en.wikipedia.org/wiki/\"\n",
    "titles = [\n",
    "    \"Integral\", \n",
    "    \"Riemann_integral\", \n",
    "    \"Riemann-Stieltjes_integral\", \n",
    "    \"Derivative\",\n",
    "    \"Limit_of_a_sequence\", \n",
    "    \"Edvard_Munch\", \n",
    "    \"Vincent_van_Gogh\", \n",
    "    \"Jan_Matejko\",\n",
    "    \"Lev_Tolstoj\", \n",
    "    \"Franz_Kafka\", \n",
    "    \"J._R._R._Tolkien\"\n",
    "]\n",
    "\n",
    "urls = [wiki_url + title for title in titles]\n",
    "\n",
    "# articles = [urlopen(url).read() for url in urls]\n",
    "articles = [requests.get(url).content for url in urls ]\n",
    "\n",
    "# Wyświetlamy pierwsze 200 znaków pierwszego artykułu\n",
    "print(articles[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ba2e4-d1a2-4315-9149-99708b0fdc03",
   "metadata": {},
   "source": [
    "Wczytaliśmy cały kod strony artykułu (włącznie z informacjami o nagłówkach, paskami nawigacyjnymi, ... ). Nas interesuje wyłącznie sama treść artykułów. W jaki sposób można wyciągnąć z kodu źródłowego strony wybrane informacje?\n",
    "\n",
    "Proces mający na celu wyciągnięcie z nieustrukturyzowanego zbioru danych tekstowych (a kod źródłowy strony internetowej jest takim zbiorem) wybranych informacji nazywany jest **scrapingiem**. W Pythonie najpopularniejszą biblioteką do scrapingu jest BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c374a-db41-448b-ab72-ad47da9a3d37",
   "metadata": {},
   "source": [
    "Żeby \"zeskrapować\" jakiś tekst, najpierw trzeba rozpoznać w nim charakterystyczne elementy. Przeglądając kod źródłowy artykułów zauważymy, że artykuły na wiki są zamknięte w elemencie `<div>` o `id=bodyContent`, a poszczególne akapity artykułu to po prostu paragrafy (`<p>`). \n",
    "\n",
    "Czyli teraz z całej tej pobranej treści chcemy wyciągnąć wyłącznie elementy `<p>` znajdujące się wewnątrz elementu `<div>` o id `\"bodyContent\"`. Do tego właśnie służy biblioteka BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c199a-01fd-4ccf-832b-c72adaeb4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "articles_paragraphs = [BeautifulSoup(article).find(\"div\", id=\"bodyContent\").find_all(\"p\") for article in articles]\n",
    "\n",
    "# wyświetlmy pierwszy paragraf pierwszego artykułu\n",
    "print(articles_paragraphs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeee56b-9788-4e22-a8e7-49ba9ea73397",
   "metadata": {},
   "source": [
    "Każdy paragraf jest reprezentowany jako obiekty klasy Tag biblioteki BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fef723-b15d-4c67-80e5-0fbbd4cb6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(articles_paragraphs[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603def1-b4ee-4525-a378-e279a1ef172f",
   "metadata": {},
   "source": [
    "Zrzutujmy paragrafy na typ string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a304715-6cb9-4fab-9491-44757f3389ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_paragraphs = [[str(paragraph) for paragraph in paragraphs] for paragraphs in articles_paragraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06871c9-b06f-4eac-b165-2127ed135284",
   "metadata": {},
   "source": [
    "Teraz każdy artykuł mamy porozbijany na zbiór paragrafów. Sklejmy te paragrafy tak, żeby artykuły znów stały się jednym ciągiem znaków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869aab7-11c3-40b8-8cf5-8e059a495a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = [\"\".join(paragraphs) for paragraphs in articles_paragraphs]\n",
    "\n",
    "# wyświetlmy pierwsze 200 znaków pierwszego, tak przetworzonego artykułu\n",
    "print(scraped_articles[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c4422-7779-4fd6-b345-0dd22ae54fe4",
   "metadata": {},
   "source": [
    "Pozbądźmy się z tekstu znaczników html, tak żeby w paragrafach został już czysty tekst. Do tego celu użyjemy biblioteki `re` (pythonowej biblioteki do pracy z wyrażeniami regularnymi).\n",
    "\n",
    "Za wzorzec znacznika html przyjmujemy `<.+?>` (znak \"<\" po którym następuje jedne lub więcej znaków, kończący się znakiem \">\" - leniwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e8c28-0ff8-4195-9ddf-a2cdc94d5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cleaned_articles = [re.sub(\"<.+?>\", \"\", article) for article in scraped_articles]\n",
    "\n",
    "# wyświetlmy piersze 5000 znaków pierwszego, oczyszczonego w ten sposób artykuł\n",
    "print(cleaned_articles[0][:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6a9f6-0c96-4391-8b98-2d14fa88fe20",
   "metadata": {},
   "source": [
    "Jeszcze trochę preporcessingu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673d40e-b64b-4cdf-85f6-0a83371e161d",
   "metadata": {},
   "source": [
    "Zamieńmy wielkie litery na małe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630311e0-c72a-43a2-8d58-e32a0bc08ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = [a.lower() for a in cleaned_articles]\n",
    "\n",
    "# wyświetlmy piersze 200 znaków pierwszego artykułu\n",
    "print(cleaned_articles[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b1663-61a9-4472-a3a2-9842ac567db9",
   "metadata": {},
   "source": [
    "Tokenizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918f43b-54a5-4f0b-81d4-8ee06772549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "cleaned_articles = [word_tokenize(article) for article in cleaned_articles]\n",
    "\n",
    "# wyświetlmy pierwsz 20 tokenów pierwszego artykułu\n",
    "print(cleaned_articles[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a8de7-b3c8-4b17-8214-c6adba963c58",
   "metadata": {},
   "source": [
    "Usuńmy znaki interpunkcyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5c224-6fd1-4de9-8c6b-facd87c4475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "cleaned_articles = [[token for token in article if token not in string.punctuation] for article in cleaned_articles]\n",
    "\n",
    "# wyświetlmy piersze 20 tokenów pierwszego artykułu\n",
    "print(cleaned_articles[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549301f-3db6-4dc6-bcb5-82d423faccbe",
   "metadata": {},
   "source": [
    "Usuwamy stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515e1f5-4252-4cf1-b6f1-7c8aad2fd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "cleaned_articles = [[token for token in article if token not in stopwords_list] for article in cleaned_articles]\n",
    "\n",
    "# wyświetlmy pierwsze 20 tokenów pierwszego artykułu\n",
    "print(cleaned_articles[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165273ee-ad83-47bf-9113-cb35ac6bbbb9",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e93df7-5bbe-4cd2-a95e-648a1a69258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  # najpopularniejszy stemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "cleaned_articles = [[stemmer.stem(token) for token in article] for article in cleaned_articles]\n",
    "\n",
    "# wyświetlmy pierwsze 20 tokenów pierwszego artykułu\n",
    "print(cleaned_articles[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1919a-b776-41ad-ab29-188dc0b1636d",
   "metadata": {},
   "source": [
    "#### Embedding (osadzanie/wektoryzacja)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d9489-4002-4c21-9efc-736256127546",
   "metadata": {},
   "source": [
    "tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed057d3-93cb-488f-a645-f584958fa751",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c14e3f-5dc0-4bcc-9028-779f115671e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "dictionary = Dictionary(cleaned_articles)\n",
    "bow_corpus = [dictionary.doc2bow(article) for article in cleaned_articles]\n",
    "tfidf_model = TfidfModel(bow_corpus)\n",
    "\n",
    "# Wyświetlmy pierwsze 100 tokenów pierwszego artykułu\n",
    "tfidf_corpus = tfidf_model[bow_corpus[0]][:100]\n",
    "print(tfidf_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4bc7d0-7172-416e-b6ce-fbabb28269f0",
   "metadata": {},
   "source": [
    "Wyświetlmy pierwsze 10 najczęściej występujących tokenów z pierwszego artykułu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5709ae-4078-43b5-bc1a-0cd5a4c8c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = sorted(tfidf_model[bow_corpus[0]], key=lambda x: x[1], reverse=True)[:10]\n",
    "first_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c4697-3420-44c9-871b-644009c21e10",
   "metadata": {},
   "source": [
    "Co to za słowa ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c6fa5-a65c-4377-b838-fbd856bb0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "[dictionary[token[0]] for token in first_article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adcbb8-16c6-4631-8c5e-9f3ddb20a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = tfidf_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528bcae-c7b8-4bf4-a464-be9adcf71b1f",
   "metadata": {},
   "source": [
    "### Analiza semantyczna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfdc3d-16d4-4c34-bbe3-3a6c5d0a7fa8",
   "metadata": {},
   "source": [
    "Jednym z kolejnych etapów może być analiza semantyczna (czyli znaczeniowa) przetwarzanych treści. Częstym zadaniem, które w tym miejscu pojawiaja się jest dopasowywanie tytułów (kategorii) do treści. Jednym z algorytmów odnoszących świetne wyniki w tej dziedzinie jest algorytm LDA (*ang. Latent Dirichlet allocation*), który do znalezienia najlepiej pasujących tytułów wykorzystuje analizę PCA podanego mu rozkładu tfidf lub bow. Biblioteka Gensim posiada implementację tego algorytmu. Wystarczy przekazać do niej słownik oraz wynik zwrócony przez model tfidf lub bow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35baa375-73fe-48c1-a15d-f1b0891c0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=tfidf_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=30,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=\"auto\"\n",
    ")\n",
    "lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698d530-3004-4dda-90e4-91497b6fa63f",
   "metadata": {},
   "source": [
    "W modelu lda przyjęliśmy wartość parametru `num_topics` 30, co oznacza, że model będzie szukał 30 najlepszych tytułów dla całego korpusu. Tytuł będzie zbudowanych z najczęściej występujących w tokenów. W pythonie istnieje biblioteka (pyldavis) do wizualizacji wyników zwróconych przez model. Wyświetlmy wynik na wykresie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08e4d0-9655-414c-9e69-5db819d1fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cadf05-c6b3-4d38-8846-fd222f8a7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64c9aa-2322-49ba-bea4-7d7ae6e35db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(\n",
    "    lda_model, \n",
    "    tfidf_corpus, \n",
    "    dictionary, \n",
    "    mds=\"mmds\", \n",
    "    R=30  # liczba tokenów branych pod uwagę podczas analizy tytułu dla wybranego artykułu\n",
    ")\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c666bcb-d2f1-43c1-834d-ae1a9acc0f95",
   "metadata": {},
   "source": [
    "Każdy ze znajdujących się po lewej stronie okręgów reprezentuje jakiś potencjalny tytuł (zbiór tokenów, z których ten tytuł można zbudować). Chcieliśmy znaleźć 30 propozycji tytułów. Algorytmowi udało się znaleźć 8. Niektóre z nich (np. 3 i 4) nakładają się. Im bardziej tytuły będą od siebie odseparowane, tym lepiej. Po kliknięciu w wybraną propozycję (okrąg) po prawej stronie zobaczymy słowa powiązane z tym tytułem i ich wagi.\n",
    "\n",
    "Wynik mógłby być lepszy. Pierwszą rzeczą, którą napewno warto zrobić pod kątem poprawienia wyniku jest usunięcie tokenów, które nie niosą ze sobą wiele znaczenia, a nie było ich na liście stopwords. W przypadku propozycji tytułu numer 1 będą to napewno tokeny takie jak 'f', 'x', 'g', 'b', '`', ... Warto też pewnie zmniejszyć liczbę słów, z których próbujemy zbudować tytuł. Zostawmy to ćwiczenie do samodzielnego rozwiązania, bo jest ono dobrym sposobem na utrwalenie materiału.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

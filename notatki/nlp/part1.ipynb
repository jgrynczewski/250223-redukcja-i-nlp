{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe9c442",
   "metadata": {},
   "source": [
    "## Tokenizacja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892a5b4-d26d-4625-87c9-d7afefe17010",
   "metadata": {},
   "source": [
    "Tokenizacja - proces przekształacania ciągu znaków lub dokumentu na mniejsze fragmenty nazywane tokenami. \n",
    "\n",
    "Tokenami często są pojedyncze słowa, ale nie jest to regułą. W zależności od kontekstu realzowanego zadania za token możemy chcieć przyjąć zdania albo pojedyncze litery. Możemy też stworzyć własne reguły tokenizacji.\n",
    "\n",
    "Przykłady zastosowania:\n",
    "- rozbicie słów lub zdań na mniejsze części,\n",
    "- odseparowanie znaków interpunkcyjnych.\n",
    "- odseparowanie hashtagów w tweecie\n",
    "- usuwanie niechcianych tokenów\n",
    "\n",
    "Tokenizacja stanowi najczęściej jeden z etapów wstępnego przetwarzanie tekstu przygotowującego tekst do dalszego przetwarzania/ustrukturyzowania.\n",
    "\n",
    "Jedną z częściej wykorzystywanych w pythonie bibliotek do tokenizacji jest [nltk](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7e908-c45f-43ba-b8f5-8e69435dcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ed5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6086a-2a6a-4efe-92ec-e7e5dd9a5b03",
   "metadata": {},
   "source": [
    "Biblioteka nltk posiada kilka tokenizerów:\n",
    "- word_tokenizer - wyodrębnia z tekstu słowa (ciągi znaków rozdzielone białymi znakami lub znakami interpunkcyjnym, przy czym znaki interpunkcji traktowane są jako odrębne słowa).\n",
    "- sent_tokenize - wyodrębnia z tekstu zdania\n",
    "- regexp_tokenize - wyodrębnia z tekstu tokeny zdefiniowane za pomocą wprowadzonego wyrażenia regularnego\n",
    "- TweetTokenizer - klasa do wyodrębniania z tekstu tweetów\n",
    "\n",
    "(you can think of TweetTokenizer as a subset of word_tokenize. TweetTokenizer keeps hashtags intact while word_tokenize doesn't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f671ff-d923-4424-9b3d-04e70e9a5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(nltk.tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6402c6c-cc8c-41ac-8e40-418ee8bf2ee6",
   "metadata": {},
   "source": [
    "#### Wyodrębnianie słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb86953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy word_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea99e7-d6e6-43a9-a258-d6b2f229d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podstawowe użyce\n",
    "res = word_tokenize(\"Hello, world!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67abcb25-fb6e-4c23-b9c2-0dee5cd9291a",
   "metadata": {},
   "source": [
    "#### Wyodrębnianie zdań"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2c4fc-e987-4984-9c93-5b3a9de1db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c430be4-0758-43be-8054-22546431287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podstawowe użycie\n",
    "sentences = \"I don't use my uncle's tools anymore. Another sentence in english.\"\n",
    "\n",
    "res = sent_tokenize(sentences)\n",
    "for sentence in res:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9a7d3-eea8-45c3-9398-9beb79352b1a",
   "metadata": {},
   "source": [
    "#### Wyodrębnienie tokena na podstawie wyrażenia regularnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc76ab-e855-4c66-9e26-07fc0e69c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujem regexp_tokenize\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podstawowe użycie\n",
    "res1 = regexp_tokenize(sentences, '\\w+')  # a-z A-Z 0-9 _\n",
    "print(res1)\n",
    "\n",
    "res2 = word_tokenize(sentences)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e3a34-27c9-4342-b590-6c25ab18adb2",
   "metadata": {},
   "source": [
    "#### Wyodrębnienie tweeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a132b16-81f2-40f5-9f6c-a20958870618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujem TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f8f28-6ac9-4052-9e90-8d1e03d13022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podstawowe użycie\n",
    "tweets = [\n",
    "    'ready? #vcut',\n",
    "    'Ohmmmmmmyyyyyyyyggghghhhhhhhgggggggggdhdhsjsixudbslsogbdsisgshdbxidjdbdidhdifjfiri #GRAMMYs #BTS',\n",
    "]\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "res = tknzr.tokenize(tweets[1])\n",
    "\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe9c442",
   "metadata": {},
   "source": [
    "## Tokenizacja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892a5b4-d26d-4625-87c9-d7afefe17010",
   "metadata": {},
   "source": [
    "Tokenizacja - proces przekształacania ciągu znaków lub dokumentu na mniejsze fragmenty nazywane tokenami. \n",
    "\n",
    "Tokenami często są pojedyncze słowa, ale nie jest to regułą. W zależności od kontekstu realzowanego zadania za token możemy chcieć przyjąć zdania albo pojedyncze litery. Możemy też stworzyć własne reguły tokenizacji.\n",
    "\n",
    "Przykłady zastosowania:\n",
    "- rozbicie słów lub zdań na mniejsze części,\n",
    "- odseparowanie znaków interpunkcyjnych.\n",
    "- odseparowanie hashtagów w tweecie\n",
    "- usuwanie niechcianych tokenów\n",
    "\n",
    "Tokenizacja stanowi najczęściej jeden z etapów wstępnego przetwarzanie tekstu przygotowującego tekst do dalszego przetwarzania/ustrukturyzowania.\n",
    "\n",
    "Jedną z częściej wykorzystywanych w pythonie bibliotek do tokenizacji jest [nltk](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da7e908-c45f-43ba-b8f5-8e69435dcc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.5-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\pycharmprojects\\250223-redukcja-i-nlp\\venv\\lib\\site-packages (from nltk) (1.3.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2023.6.3-cp311-cp311-win_amd64.whl (268 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\pycharmprojects\\250223-redukcja-i-nlp\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.5 nltk-3.8.1 regex-2023.6.3 tqdm-4.65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7ed5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6086a-2a6a-4efe-92ec-e7e5dd9a5b03",
   "metadata": {},
   "source": [
    "Biblioteka nltk posiada kilka tokenizerów:\n",
    "- word_tokenizer - wyodrębnia z tekstu słowa (ciągi znaków rozdzielone białymi znakami lub znakami interpunkcyjnym, przy czym znaki interpunkcji traktowane są jako odrębne słowa).\n",
    "- sent_tokenize - wyodrębnia z tekstu zdania\n",
    "- regexp_tokenize - wyodrębnia z tekstu tokeny zdefiniowane za pomocą wprowadzonego wyrażenia regularnego\n",
    "- TweetTokenizer - klasa do wyodrębniania z tekstu tweetów\n",
    "\n",
    "(you can think of TweetTokenizer as a subset of word_tokenize. TweetTokenizer keeps hashtags intact while word_tokenize doesn't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f671ff-d923-4424-9b3d-04e70e9a5f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BlanklineTokenizer', 'LegalitySyllableTokenizer', 'LineTokenizer', 'MWETokenizer', 'NLTKWordTokenizer', 'PunktSentenceTokenizer', 'RegexpTokenizer', 'ReppTokenizer', 'SExprTokenizer', 'SpaceTokenizer', 'StanfordSegmenter', 'SyllableTokenizer', 'TabTokenizer', 'TextTilingTokenizer', 'ToktokTokenizer', 'TreebankWordDetokenizer', 'TreebankWordTokenizer', 'TweetTokenizer', 'WhitespaceTokenizer', 'WordPunctTokenizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_treebank_word_tokenizer', 'api', 'blankline_tokenize', 'casual', 'casual_tokenize', 'destructive', 'legality_principle', 'line_tokenize', 'load', 'mwe', 'punkt', 're', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'repp', 'sent_tokenize', 'sexpr', 'sexpr_tokenize', 'simple', 'sonority_sequencing', 'stanford_segmenter', 'string_span_tokenize', 'texttiling', 'toktok', 'treebank', 'util', 'word_tokenize', 'wordpunct_tokenize']\n"
     ]
    }
   ],
   "source": [
    "print(dir(nltk.tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6402c6c-cc8c-41ac-8e40-418ee8bf2ee6",
   "metadata": {},
   "source": [
    "#### Wyodrębnianie słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb86953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy word_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ea99e7-d6e6-43a9-a258-d6b2f229d9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użyce\n",
    "res = word_tokenize(\"Hello, world!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67abcb25-fb6e-4c23-b9c2-0dee5cd9291a",
   "metadata": {},
   "source": [
    "#### Wyodrębnianie zdań"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b2c4fc-e987-4984-9c93-5b3a9de1db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c430be4-0758-43be-8054-22546431287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't use my uncle's tools anymore.\n",
      "Another sentence in english.\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użycie\n",
    "sentences = \"I don't use my uncle's tools anymore. Another sentence in english.\"\n",
    "\n",
    "res = sent_tokenize(sentences)\n",
    "for sentence in res:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9a7d3-eea8-45c3-9398-9beb79352b1a",
   "metadata": {},
   "source": [
    "#### Wyodrębnienie tokena na podstawie wyrażenia regularnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cc76ab-e855-4c66-9e26-07fc0e69c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujem regexp_tokenize\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee9f1558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'don', 't', 'use', 'my', 'uncle', 's', 'tools', 'anymore', 'Another', 'sentence', 'in', 'english']\n",
      "['I', 'do', \"n't\", 'use', 'my', 'uncle', \"'s\", 'tools', 'anymore', '.', 'Another', 'sentence', 'in', 'english', '.']\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użycie\n",
    "res1 = regexp_tokenize(sentences, '\\w+')  # a-z A-Z 0-9 _\n",
    "print(res1)\n",
    "\n",
    "res2 = word_tokenize(sentences)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e3a34-27c9-4342-b590-6c25ab18adb2",
   "metadata": {},
   "source": [
    "#### Wyodrębnienie tweeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a132b16-81f2-40f5-9f6c-a20958870618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujem TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be5f8f28-6ac9-4052-9e90-8d1e03d13022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ready', '?', '#vcut'], ['Ohmmmmmmyyyyyyyyggghghhhhhhhgggggggggdhdhsjsixudbslsogbdsisgshdbxidjdbdidhdifjfiri', '#GRAMMYs', '#BTS']]\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użycie\n",
    "tweets = [\n",
    "    'ready? #vcut',\n",
    "    'Ohmmmmmmyyyyyyyyggghghhhhhhhgggggggggdhdhsjsixudbslsogbdsisgshdbxidjdbdidhdifjfiri #GRAMMYs #BTS',\n",
    "]\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "res = tknzr.tokenize(tweets[1])\n",
    "\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
